<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>TIM: A Time Interval Machine for Audio-Visual Action Recognition</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="TIM: A Time Interval Machine for Audio-Visual Action Recognition" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://jacobchalk.github.io/TIM-Project/" />
<meta property="og:url" content="https://jacobchalk.github.io/TIM-Project/" />
<meta property="og:site_name" content="TIM: A Time Interval Machine for Audio-Visual Action Recognition" />
<script type="application/ld+json">
{"@type":"WebSite","headline":"TIM: A Time Interval Machine for Audio-Visual Action Recognition","url":"https://jacobchalk.github.io/TIM-Project/","name":"TIM: A Time Interval Machine for Audio-Visual Action Recognition","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">TIM: A Time Interval Machine for Audio-Visual Action Recognition (CVPR 2024)</h1>
      <h2 class="project-tagline"></h2>


    </section>

    <section class="main-content">
      <p align="center" style="font-size:30px">
        <a href="https://jacobchalk.github.io/">Jacob Chalk</a><sup>*1</sup>, <a href="https://www.robots.ox.ac.uk/~jaesung/">Jaesung Huh</a><sup>*2</sup>, <a href="https://ekazakos.github.io/">Evangelos Kazakos</a><sup>3</sup>, <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a><sup>2</sup> and <a href="https://dimadamen.github.io/">Dima Damen</a><sup>1</sup>
</p>
<p align="center" style="font-size:20px">
    <sup>1</sup>University of Bristol, Dept. of Computer Science, <sup>2</sup>University of Oxford, VGG, <sup>3</sup>Czech Technical University in Prague
</p>
<p align="center" style="font-size:20px">
    <sup>*</sup>: Indicates equal contribution
</p>

<p><img src="/TIM-Project/teaser.png" alt="Overview"/></p>
<h2 id="abstract">Abstract</h2>
<p>Diverse actions give rise to rich audio-visual signals in long videos.
    Recent works showcase that the two modalities of audio and video exhibit different temporal extents of events and distinct labels.
    We address the interplay between the two modalities in long videos by explicitly modelling the temporal extents of audio and visual events.
    We propose the Time Interval Machine (TIM) where a modality-specific time interval poses as a query to a transformer encoder that ingests a long video input.
    The encoder then attends to the specified interval, as well as the surrounding context in both modalities, in order to recognise the ongoing action.
</p>

<p>We test TIM on three long audio-visual video datasets: EPIC-KITCHENS, Perception Test, and AVE, reporting state-of-the-art (SOTA) for recognition.
    On EPIC-KITCHENS, we beat previous SOTA that utilises LLMs and significantly larger pre-training by 2.9% top-1 action recognition accuracy.
    Additionally, we show that TIM can be adapted for action detection, using dense multi-scale interval queries, outperforming SOTA on EPIC-KITCHENS-100 for most metrics, and showing strong performance on the Perception Test.
    Our ablations show the critical role of integrating the two modalities and modelling their time intervals in achieving this performance. Code and models at: <a href="https://github.com/JacobChalk/TIM">https://github.com/JacobChalk/TIM</a>.</p>

<h2 id="video">Video</h2>

<style>
    .embed-container {
        position: relative;
        padding-bottom: 56.25%;
        height: 0;
        overflow: hidden;
        max-width: 100%;
    }
    .embed-container iframe,
    .embed-container object,
    .embed-container embed {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
    }
</style>

<div class="embed-container">
    <iframe src="https://www.youtube.com/embed/uEWc5EpZJg4" frameborder="0" allowfullscreen=""></iframe>
</div>

<h2 id="paper">Downloads</h2>
<ul>
<li>Paper <a class="nounderline" target="_blank" href="https://arxiv.org/abs/2404.05559">[arXiv]</a></li>
<li>Code and models <a class="nounderline" target="_blank" href="https://github.com/JacobChalk/TIM">[GitHub]</a></li>
</ul>

<h2 id="bibtex">Bibtex</h2>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
@InProceedings{Chalk_2024_CVPR,
    author    = {Chalk, Jacob and Huh, Jaesung and Kazakos, Evangelos and Zisserman, Andrew and Damen, Dima},
    title     = {TIM: A Time Interval Machine for Audio-Visual Action Recognition},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {18153-18163}
}

</code></pre></div></div>

<h2 id="acknowledgements">Acknowledgements</h2>
<p>This work uses public datasets. It is supported by EPSRC Doctoral Training Program, EPSRC UMPIRE EP/T004991/1 and EPSRC Programme Grant VisualAI EP/T028572/1; and by the use of the EPSRC funded Tier 2 facility JADE-II.</p>


      <footer class="site-footer">

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </section>


  </body>
</html>

